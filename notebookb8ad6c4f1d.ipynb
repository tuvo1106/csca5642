{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:42.250715Z","iopub.execute_input":"2025-09-26T16:13:42.251072Z","iopub.status.idle":"2025-09-26T16:13:42.260333Z","shell.execute_reply.started":"2025-09-26T16:13:42.251048Z","shell.execute_reply":"2025-09-26T16:13:42.259139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import graphviz\nimport keras\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport re\nimport random\nimport seaborn as sns\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nfrom tensorflow.keras import Input\nfrom keras.layers import Activation, Dense, Dropout\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom matplotlib import rcParams\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import (\n    BaggingClassifier,\n    ExtraTreesClassifier,\n    GradientBoostingClassifier,\n    RandomForestClassifier,\n    RandomTreesEmbedding,\n    VotingClassifier,\n)\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import (\n    KFold,\n    StratifiedKFold,\n    cross_val_score,\n    cross_validate,\n    train_test_split,\n)\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom xgboost import XGBClassifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:42.262301Z","iopub.execute_input":"2025-09-26T16:13:42.262694Z","iopub.status.idle":"2025-09-26T16:13:42.302868Z","shell.execute_reply.started":"2025-09-26T16:13:42.262670Z","shell.execute_reply":"2025-09-26T16:13:42.301655Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Project Topic\n\nThis project is about trying to predict passenger survivorship based on data about the passengers on the Titanic.\nI will be utilizing Neural Networks to create a model that makes the prediction. This is a logistic regression task.","metadata":{}},{"cell_type":"markdown","source":"## Project Goals\n\nThe goal of the project is to have practice with Neural Networks and develop my skills with hyperparameter tuning. As a history lover and tech enthusiast, this dataset will be fun to work with and provide an opportunity for me to compare many different Machine Learning algorithms. I want to see side-by-side how Deep Learning compares to Supervised/Unsupervised Learning in small datasets like this one.","metadata":{}},{"cell_type":"markdown","source":"# Data\n\nSource: https://www.kaggle.com/competitions/titanic/data\n\nThis data is public and comes from Kaggle.","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/titanic/train.csv\")\ndf_test = pd.read_csv(\"../input/titanic/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:42.303618Z","iopub.execute_input":"2025-09-26T16:13:42.303949Z","iopub.status.idle":"2025-09-26T16:13:42.340498Z","shell.execute_reply.started":"2025-09-26T16:13:42.303915Z","shell.execute_reply":"2025-09-26T16:13:42.339251Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Info and Size","metadata":{}},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:42.342741Z","iopub.execute_input":"2025-09-26T16:13:42.343164Z","iopub.status.idle":"2025-09-26T16:13:42.357835Z","shell.execute_reply.started":"2025-09-26T16:13:42.343137Z","shell.execute_reply":"2025-09-26T16:13:42.356681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:42.358857Z","iopub.execute_input":"2025-09-26T16:13:42.359169Z","iopub.status.idle":"2025-09-26T16:13:42.403117Z","shell.execute_reply.started":"2025-09-26T16:13:42.359131Z","shell.execute_reply":"2025-09-26T16:13:42.402016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train.head(25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:42.404382Z","iopub.execute_input":"2025-09-26T16:13:42.404707Z","iopub.status.idle":"2025-09-26T16:13:42.424337Z","shell.execute_reply.started":"2025-09-26T16:13:42.404682Z","shell.execute_reply":"2025-09-26T16:13:42.423210Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:42.425671Z","iopub.execute_input":"2025-09-26T16:13:42.425992Z","iopub.status.idle":"2025-09-26T16:13:42.454380Z","shell.execute_reply.started":"2025-09-26T16:13:42.425967Z","shell.execute_reply":"2025-09-26T16:13:42.453319Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nI will now plot distribuion of values in all categories as well as their relation to the target variable.","metadata":{}},{"cell_type":"code","source":"def plot_column_values(df, max_unique=15, skip_high_cardinality=True):\n    \"\"\"\n    Plots value distributions for each column in the dataframe.\n    - For numeric columns: histogram\n    - For categorical/low-cardinality columns: countplot\n    - Skips high-cardinality columns like Name/Ticket if skip_high_cardinality=True\n    \"\"\"\n    cols_to_plot = []\n    for col in df.drop(columns=[\"PassengerId\"]).columns:\n        if skip_high_cardinality and df[col].nunique() > max_unique and df[col].dtype == \"object\":\n            continue\n        cols_to_plot.append(col)\n\n    n_cols = 3\n    n_rows = int(np.ceil(len(cols_to_plot) / n_cols))\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5 * n_rows))\n    axes = axes.flatten()\n\n    for i, col in enumerate(cols_to_plot):\n        ax = axes[i]\n        if df[col].dtype == \"object\" or df[col].nunique() <= max_unique:\n            sns.countplot(x=col, data=df, ax=ax, order=df[col].value_counts().index)\n            ax.set_title(f\"{col} (countplot)\")\n            ax.tick_params(axis=\"x\", rotation=45)\n        else:\n            sns.histplot(df[col].dropna(), bins=30, kde=False, ax=ax)\n            ax.set_title(f\"{col} (histogram)\")\n\n    for j in range(i + 1, len(axes)):\n        fig.delaxes(axes[j])\n\n    plt.tight_layout()\n    plt.show()\n\nplot_column_values(df_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:42.455101Z","iopub.execute_input":"2025-09-26T16:13:42.455389Z","iopub.status.idle":"2025-09-26T16:13:44.358084Z","shell.execute_reply.started":"2025-09-26T16:13:42.455356Z","shell.execute_reply":"2025-09-26T16:13:44.357075Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Survived**\n\nWhen I look at the Survived column, I see that more passengers did not survive than those who did. This tells me the dataset is imbalanced, with survival being the minority class.\n\n**Pclass**\n\nThe distribution of Pclass shows that most passengers were in 3rd class, while fewer were in 1st class. This suggests that socio-economic status could have had an impact on survival.\n\n**Sex**\n\nThe Sex column is roughly balanced between male and female, but slightly more males are present. Since gender is known to affect survival rates, I expect this feature to be highly predictive.\n\n**Age**\n\nThe Age histogram shows a concentration of passengers in their 20s and 30s, with a long tail into older ages. There are also some children on board, which might influence survival differently compared to adults.\n\n**SibSp**\n\nMost passengers traveled alone or with one sibling/spouse. Very few had large numbers of siblings or spouses, which suggests that being part of a large group was uncommon.\n\n**Parch**\n\nThe Parch column shows that most passengers did not have parents or children with them. A smaller group traveled with one or two relatives, while large families were rare.\n\n**Fare**\n\nThe Fare histogram is right-skewed, with most fares being low and a few passengers paying very high prices. This reflects the socio-economic diversity of the passengers and may correlate with class and survival.\n\n**Embarked**\n\nMost passengers embarked from port 'S', with fewer from 'C' and 'Q'. This could be linked to geographic or socio-economic differences among passengers.\n\n**Cabin**\n\nThe Cabin column has many missing values, but among those recorded, cabins cluster around certain letters like C, D, and E. This might reflect different sections of the ship, which could relate to survival.","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"whitegrid\")\n\nplt.figure(figsize=(6,4))\nsns.countplot(x=\"Survived\", data=df_train)\nplt.title(\"Overall Survival Distribution\")\nplt.show()\n\nplt.figure(figsize=(6,4))\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=df_train)\nplt.title(\"Survival Rate by Passenger Class\")\nplt.show()\n\nplt.figure(figsize=(6,4))\nsns.barplot(x=\"Sex\", y=\"Survived\", data=df_train)\nplt.title(\"Survival Rate by Sex\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nsns.histplot(df_train[df_train[\"Survived\"]==1][\"Age\"].dropna(), bins=30, kde=False, color=\"green\", label=\"Survived\")\nsns.histplot(df_train[df_train[\"Survived\"]==0][\"Age\"].dropna(), bins=30, kde=False, color=\"red\", label=\"Did Not Survive\")\nplt.legend()\nplt.title(\"Age Distribution by Survival\")\nplt.show()\n\nplt.figure(figsize=(6,4))\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=df_train)\nplt.title(\"Survival Rate by Number of Siblings/Spouses Aboard\")\nplt.show()\n\nplt.figure(figsize=(6,4))\nsns.barplot(x=\"Parch\", y=\"Survived\", data=df_train)\nplt.title(\"Survival Rate by Number of Parents/Children Aboard\")\nplt.show()\n\nplt.figure(figsize=(8,5))\nsns.boxplot(x=\"Survived\", y=\"Fare\", data=df_train)\nplt.ylim(0, 100)\nplt.title(\"Fare vs Survival (zoomed)\")\nplt.show()\n\nplt.figure(figsize=(6,4))\nsns.barplot(x=\"Embarked\", y=\"Survived\", data=df_train)\nplt.title(\"Survival Rate by Port of Embarkation\")\nplt.show()\n\nif \"Cabin\" in df_train.columns:\n    df_train[\"CabinLetter\"] = df_train[\"Cabin\"].fillna(\"U\").str[0]\n    plt.figure(figsize=(8,4))\n    sns.barplot(x=\"CabinLetter\", y=\"Survived\", data=df_train)\n    plt.title(\"Survival Rate by Cabin Letter\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:44.359055Z","iopub.execute_input":"2025-09-26T16:13:44.359372Z","iopub.status.idle":"2025-09-26T16:13:47.400129Z","shell.execute_reply.started":"2025-09-26T16:13:44.359349Z","shell.execute_reply":"2025-09-26T16:13:47.398809Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Overall Survival Distribution**\n\nWhen I looked at the overall survival distribution, I noticed that the majority of passengers did not survive. This highlights a class imbalance in the dataset, which I’ll need to consider when building models.\n\n**Survival Rate by Passenger Class**\n\nThe survival rate was highest in 1st class and lowest in 3rd class. This suggests that socio-economic status strongly influenced who survived.\n\n**Survival Rate by Sex**\n\nThe survival rate for females was much higher than for males. This supports the historical reports of “women and children first” during evacuation.\n\n**Age Distribution by Survival**\n\nFrom the age distribution, I see that most passengers were in their twenties and thirties. Younger children had better survival rates, while older passengers were less likely to survive.\n\n**Survival Rate by Number of Siblings/Spouses Aboard (SibSp)**\n\nPassengers with one or two siblings/spouses on board had the best survival chances. Being alone or having a large number of relatives reduced survival odds.\n\n**Survival Rate by Number of Parents/Children Aboard (Parch)**\n\nModerate family groups (1–2 parents/children) had higher survival rates. Passengers traveling alone or in large families had lower chances of survival.\n\n**Fare vs Survival (Boxplot)**\n\nSurvivors generally paid higher fares compared to those who did not survive. This aligns with the earlier finding that wealthier passengers had better survival odds.\n\n**Survival Rate by Port of Embarkation**\n\nPassengers who boarded at Cherbourg (C) had the highest survival rate. Those embarking from Southampton (S) had the lowest, which might reflect differences in class or accommodations.\n\n**Survival Rate by Cabin Letter**\n\nCabins in sections like B, C, and E had higher survival rates, while unknown cabins (U) had very low survival. This suggests that cabin location on the ship was an important factor.","metadata":{}},{"cell_type":"markdown","source":"## Data Preprocessing/Cleaning\n\nNext, we will clean the data. I will do the following:\n- drop irrelevant columns like 'PassengerId'\n- extract title from name\n- fill in missing values with median/modes\n\nThis process, also called imputation, helps our model be more accurate because a lot of Machine Learning algorithms require complete datasets. Most libraries will throw errors or exclude missing rows so we can be potentially missing out on a lot of valuable observations.","metadata":{}},{"cell_type":"markdown","source":"Next, let's look at some of data distributions now that they have been cleaned.","metadata":{}},{"cell_type":"code","source":"def _extract_and_normalize_title(s):\n    t = s.str.extract(r\" ([A-Za-z]+)\\.\", expand=False)\n    t = t.replace([\"Mlle\", \"Ms\"], \"Miss\").replace(\"Mme\", \"Mrs\")\n    t = t.replace(\n        [\"Lady\", \"Countess\", \"Capt\", \"Col\", \"Don\", \"Dr\", \"Major\", \n         \"Rev\", \"Sir\", \"Jonkheer\", \"Dona\"],\n        \"Rare\"\n    )\n    return t\n\ndef preprocess_titanic(df_train: pd.DataFrame, df_test: pd.DataFrame):\n    tr = df_train.copy()\n    te = df_test.copy()\n\n    for d in (tr, te):\n        if \"PassengerId\" in d.columns:\n            d.drop(columns=[\"PassengerId\"], inplace=True)\n\n    tr[\"Title\"] = _extract_and_normalize_title(tr[\"Name\"])\n    te[\"Title\"] = _extract_and_normalize_title(te[\"Name\"])\n\n    embarked_mode = tr[\"Embarked\"].mode().iloc[0] if tr[\"Embarked\"].notna().any() else \"S\"\n    fare_median   = tr[\"Fare\"].median() if tr[\"Fare\"].notna().any() else 0.0\n\n    age_by_title = tr.groupby(\"Title\")[\"Age\"].median()\n    overall_age_median = tr[\"Age\"].median() if tr[\"Age\"].notna().any() else 30.0\n\n    tr[\"Embarked\"] = tr[\"Embarked\"].fillna(embarked_mode)\n    te[\"Embarked\"] = te[\"Embarked\"].fillna(embarked_mode)\n\n    tr[\"Fare\"] = tr[\"Fare\"].fillna(fare_median)\n    te[\"Fare\"] = te[\"Fare\"].fillna(fare_median)\n\n    tr[\"Cabin\"] = tr[\"Cabin\"].fillna(\"U\").astype(str).str[0]\n    te[\"Cabin\"] = te[\"Cabin\"].fillna(\"U\").astype(str).str[0]\n\n    def fill_age_with_train_stats(df):\n        df[\"Age\"] = df[\"Age\"].copy()\n        df.loc[df[\"Age\"].isna(), \"Age\"] = (\n            df.loc[df[\"Age\"].isna(), \"Title\"].map(age_by_title)\n        )\n        df[\"Age\"] = df[\"Age\"].fillna(overall_age_median)\n        return df\n\n    tr = fill_age_with_train_stats(tr)\n    te = fill_age_with_train_stats(te)\n\n    tr.drop(columns=[\"Name\", \"Ticket\"], inplace=True, errors=\"ignore\")\n    te.drop(columns=[\"Name\", \"Ticket\"], inplace=True, errors=\"ignore\")\n\n    return tr, te\n\ndf_train, df_test = preprocess_titanic(df_train, df_test)\n\nprint(df_train.head())\nprint()\nprint(df_test.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:47.404555Z","iopub.execute_input":"2025-09-26T16:13:47.404899Z","iopub.status.idle":"2025-09-26T16:13:47.465191Z","shell.execute_reply.started":"2025-09-26T16:13:47.404857Z","shell.execute_reply":"2025-09-26T16:13:47.463977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df_train[\"Title\"].value_counts())\n\nplt.figure(figsize=(8,5))\nsns.countplot(x=\"Title\", data=df_train, order=df_train[\"Title\"].value_counts().index)\nplt.title(\"Distribution of Passenger Titles\")\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:47.466253Z","iopub.execute_input":"2025-09-26T16:13:47.466603Z","iopub.status.idle":"2025-09-26T16:13:47.685142Z","shell.execute_reply.started":"2025-09-26T16:13:47.466580Z","shell.execute_reply":"2025-09-26T16:13:47.683914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.barplot(x=\"Title\", y=\"Survived\", data=df_train,\n            order=df_train[\"Title\"].value_counts().index)\nplt.title(\"Survival Rate by Passenger Title\")\nplt.ylabel(\"Survival Rate\")\nplt.xticks(rotation=45)\nplt.show()\n\nsurvival_by_title = df_train.groupby(\"Title\")[\"Survived\"].mean().sort_values(ascending=False)\nprint(survival_by_title)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:47.686158Z","iopub.execute_input":"2025-09-26T16:13:47.686487Z","iopub.status.idle":"2025-09-26T16:13:47.995951Z","shell.execute_reply.started":"2025-09-26T16:13:47.686463Z","shell.execute_reply":"2025-09-26T16:13:47.994543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"title_counts = df_train[\"Title\"].value_counts()\nsurvival_by_title = df_train.groupby(\"Title\")[\"Survived\"].mean()\norder = title_counts.index\n\nfig, ax1 = plt.subplots(figsize=(8,5))\n\nsns.barplot(x=title_counts.index, y=title_counts.values, ax=ax1, color=\"skyblue\")\nax1.set_ylabel(\"Passenger Count\")\nax1.set_xlabel(\"Title\")\nax1.set_title(\"Passenger Counts and Survival Rates by Title\")\n\nax2 = ax1.twinx()\nsns.pointplot(x=survival_by_title.index, y=survival_by_title.values,\n              order=order, ax=ax2, color=\"red\", markers=\"o\", linestyles=\"-\")\nax2.set_ylabel(\"Survival Rate\")\n\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:47.997062Z","iopub.execute_input":"2025-09-26T16:13:47.997352Z","iopub.status.idle":"2025-09-26T16:13:48.374195Z","shell.execute_reply.started":"2025-09-26T16:13:47.997329Z","shell.execute_reply":"2025-09-26T16:13:48.373102Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"When I looked at the passenger counts and survival rates by title, I noticed that most passengers were men with the title “Mr,” but they had the lowest survival rate. Titles like “Miss,” “Mrs,” and “Master” showed much higher survival, which supports the “women and children first” evacuation rule. The “Rare” group was small in size, and its survival rate was inconsistent, likely because there were so few passengers in this category.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.heatmap(df_train.corr(numeric_only=True), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\nplt.title(\"Correlation Heatmap of Numeric Features\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:48.375459Z","iopub.execute_input":"2025-09-26T16:13:48.375749Z","iopub.status.idle":"2025-09-26T16:13:48.744392Z","shell.execute_reply.started":"2025-09-26T16:13:48.375726Z","shell.execute_reply":"2025-09-26T16:13:48.743399Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The correlation heatmap shows that Survived is negatively correlated with Pclass and positively correlated with Fare. This makes sense because wealthier passengers who paid higher fares and were in higher classes had better survival chances. I also noticed that SibSp and Parch are moderately correlated with each other, since they both describe family relationships on board. Interestingly, Age has only weak correlations with survival and other features, which means it may not be as strong a predictor by itself.","metadata":{}},{"cell_type":"code","source":"df_train[\"FamilySize\"] = df_train[\"SibSp\"] + df_train[\"Parch\"] + 1\ndf_train[\"IsAlone\"] = (df_train[\"FamilySize\"] == 1).astype(int)\n\nplt.figure(figsize=(6,4))\nsns.barplot(x=\"FamilySize\", y=\"Survived\", data=df_train)\nplt.title(\"Survival Rate by Family Size\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:48.745471Z","iopub.execute_input":"2025-09-26T16:13:48.745759Z","iopub.status.idle":"2025-09-26T16:13:49.176171Z","shell.execute_reply.started":"2025-09-26T16:13:48.745738Z","shell.execute_reply":"2025-09-26T16:13:49.175116Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"When it comes to survival by family size, passengers traveling with small families (2–4 members) had the highest survival rates, often above 50%. In contrast, those traveling alone had lower chances of survival, while very large families (5 or more) also showed poor survival rates. This suggests that being with a small group provided social support, but traveling alone or with too many dependents reduced survival chances.","metadata":{}},{"cell_type":"code","source":"df_train[\"AgeBin\"] = pd.cut(\n    df_train[\"Age\"], bins=[0,12,18,35,60,80], \n    labels=[\"Child\",\"Teen\",\"Young Adult\",\"Adult\",\"Senior\"]\n)\n\nplt.figure(figsize=(6,4))\nsns.barplot(x=\"AgeBin\", y=\"Survived\", data=df_train)\nplt.title(\"Survival Rate by Age Group\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:49.177367Z","iopub.execute_input":"2025-09-26T16:13:49.177616Z","iopub.status.idle":"2025-09-26T16:13:49.499820Z","shell.execute_reply.started":"2025-09-26T16:13:49.177597Z","shell.execute_reply":"2025-09-26T16:13:49.498644Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In regards to survival by age group, children had the highest survival rate, with more than half surviving. Teenagers and adults had moderate survival chances, while young adults had the lowest among these middle groups. Seniors had the lowest survival rate overall, suggesting that age played a clear role in survival outcomes.","metadata":{}},{"cell_type":"code","source":"df_train[\"FareBin\"] = pd.qcut(df_train[\"Fare\"], 4, labels=[\"Low\",\"Medium\",\"High\",\"Very High\"])\n\nplt.figure(figsize=(6,4))\nsns.barplot(x=\"FareBin\", y=\"Survived\", data=df_train)\nplt.title(\"Survival Rate by Fare Quartiles\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:49.500860Z","iopub.execute_input":"2025-09-26T16:13:49.501142Z","iopub.status.idle":"2025-09-26T16:13:49.798811Z","shell.execute_reply.started":"2025-09-26T16:13:49.501116Z","shell.execute_reply":"2025-09-26T16:13:49.797281Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As expected, survival rates by fare quartiles revealed a clear upward trend: passengers who paid more for their tickets had higher chances of survival. Those in the lowest fare group had survival rates under 25%, while the very high fare group had survival rates close to 60%. This finding matches the earlier correlations and reinforces that wealth and class strongly influenced survival.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.barplot(x=\"Embarked\", y=\"Survived\", hue=\"Pclass\", data=df_train)\nplt.title(\"Survival by Embarked Port and Passenger Class\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:49.800084Z","iopub.execute_input":"2025-09-26T16:13:49.800337Z","iopub.status.idle":"2025-09-26T16:13:50.236159Z","shell.execute_reply.started":"2025-09-26T16:13:49.800318Z","shell.execute_reply":"2025-09-26T16:13:50.235279Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"When I analyzed survival by port of embarkation and passenger class, I noticed that 1st-class passengers from Cherbourg had the highest survival rates, often around 70%. In contrast, 3rd-class passengers from Southampton had the lowest survival, with less than 20% surviving. Passengers from Queenstown showed mixed results due to smaller sample sizes, but survival was generally higher for 2nd-class than 3rd-class. This suggests that both where passengers boarded and their socio-economic class influenced survival chances.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,4))\nsns.barplot(x=\"Cabin\", y=\"Survived\", data=df_train, \n            order=df_train[\"Cabin\"].value_counts().index)\nplt.title(\"Survival Rate by Cabin Deck\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:50.237069Z","iopub.execute_input":"2025-09-26T16:13:50.237327Z","iopub.status.idle":"2025-09-26T16:13:50.654664Z","shell.execute_reply.started":"2025-09-26T16:13:50.237305Z","shell.execute_reply":"2025-09-26T16:13:50.653363Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Lastly, when it comes to survival by cabin deck, passengers on decks B, C, D, and E had the highest survival rates, often above 70%. In contrast, passengers with unknown cabins (U) or on lower decks like A had much lower chances of survival. Decks with fewer passengers, such as G and T, showed unstable rates because of very small sample sizes. This suggests that cabin location on the ship played a role in survival, likely because some decks had easier access to lifeboats.","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering\n\nHere, I will utilize one-hot encoding to convert categorical data into numbers since Neural Networks can only work with numerical input.","metadata":{}},{"cell_type":"code","source":"def feature_engineering(tr: pd.DataFrame, te: pd.DataFrame):\n    for d in (tr, te):\n        d[\"FamilySize\"] = d[\"SibSp\"] + d[\"Parch\"] + 1\n        d[\"IsAlone\"] = (d[\"FamilySize\"] == 1).astype(int)\n        d[\"Fare_Per_Person\"] = d[\"Fare\"] / d[\"FamilySize\"]\n\n    sex_map = {\"male\": 0, \"female\": 1}\n    for d in (tr, te):\n        d[\"Sex\"] = d[\"Sex\"].map(sex_map)\n\n    tr = pd.get_dummies(tr, columns=[\"Embarked\"], drop_first=True)\n    te = pd.get_dummies(te, columns=[\"Embarked\"], drop_first=True)\n    te = te.reindex(columns=tr.columns, fill_value=0)\n\n    age_bins  = [0, 12, 18, 35, 60, 80]\n    age_labels = [\"Child\",\"Teen\",\"Young Adult\",\"Adult\",\"Senior\"]\n    for d in (tr, te):\n        d[\"AgeBin\"] = pd.cut(d[\"Age\"], bins=age_bins, labels=age_labels)\n\n    fare_bins = pd.qcut(tr[\"Fare\"], 4, labels=[\"Low\",\"Medium\",\"High\",\"Very High\"])\n    fare_cuts = pd.qcut(tr[\"Fare\"], 4, retbins=True)[1]\n    for d in (tr, te):\n        d[\"FareBin\"] = pd.cut(d[\"Fare\"], bins=fare_cuts, \n                              labels=[\"Low\",\"Medium\",\"High\",\"Very High\"], include_lowest=True)\n\n    tr = pd.get_dummies(tr, columns=[\"Title\",\"Cabin\",\"AgeBin\",\"FareBin\"], drop_first=True)\n    te = pd.get_dummies(te, columns=[\"Title\",\"Cabin\",\"AgeBin\",\"FareBin\"], drop_first=True)\n    te = te.reindex(columns=tr.columns, fill_value=0)\n\n    drop_cols = [\"CabinLetter\", \"SibSp\", \"Parch\"]\n    tr.drop(columns=[c for c in drop_cols if c in tr.columns], inplace=True)\n    te.drop(columns=[c for c in drop_cols if c in te.columns], inplace=True)\n\n    return tr, te\n\ndf_train, df_test = feature_engineering(df_train, df_test)\n\nprint(df_train.head())\nprint()\nprint(df_test.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:50.655673Z","iopub.execute_input":"2025-09-26T16:13:50.656027Z","iopub.status.idle":"2025-09-26T16:13:50.720483Z","shell.execute_reply.started":"2025-09-26T16:13:50.655998Z","shell.execute_reply":"2025-09-26T16:13:50.719486Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, I will just do some final cleaning such as coercing types and ensuring the columns are identical before we train the model.","metadata":{}},{"cell_type":"code","source":"def finalize_for_model(tr: pd.DataFrame, te: pd.DataFrame):\n    tr = tr.copy(); te = te.copy()\n\n    assert list(tr.columns) == list(te.columns), \"Train/Test columns misaligned.\"\n\n    dummy_prefixes = (\"Embarked_\", \"Title_\", \"Cabin_\", \"AgeBin_\", \"FareBin_\")\n    dummy_cols = [c for c in tr.columns if c.startswith(dummy_prefixes)]\n\n    for d in (tr, te):\n        bool_cols = d.select_dtypes(include=\"bool\").columns\n        d[bool_cols] = d[bool_cols].astype(\"uint8\")\n        d[dummy_cols] = d[dummy_cols].astype(\"uint8\")\n\n        cast_map = {\n            \"Pclass\": \"int8\",\n            \"Sex\": \"int8\",\n            \"FamilySize\": \"int16\",\n            \"IsAlone\": \"uint8\",\n        }\n        for col, dt in cast_map.items():\n            if col in d.columns:\n                d[col] = d[col].astype(dt)\n\n        float_cols = [\"Age\", \"Fare\", \"Fare_Per_Person\"]\n        for col in float_cols:\n            if col in d.columns:\n                d[col] = d[col].astype(\"float32\")\n\n    na_tr = tr.isna().sum().sum()\n    na_te = te.isna().sum().sum()\n    if na_tr or na_te:\n        raise ValueError(f\"NaNs remain — train: {na_tr}, test: {na_te}\")\n\n    return tr, te\n\ndf_train, df_test = finalize_for_model(df_train, df_test)\n\nprint(df_train.head())\nprint()\nprint(df_test.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:50.721382Z","iopub.execute_input":"2025-09-26T16:13:50.721660Z","iopub.status.idle":"2025-09-26T16:13:50.767401Z","shell.execute_reply.started":"2025-09-26T16:13:50.721638Z","shell.execute_reply":"2025-09-26T16:13:50.766417Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's look at what features are worth keeping by ranking their importance with a RandomForestClassifier.\n\nRandom forests rank feature importance by:\n- How much features reduce impurity when splitting nodes (MDI).\n- How much model accuracy drops when features are shuffled (MDA).","metadata":{}},{"cell_type":"code","source":"X = df_train.drop(\"Survived\", axis=1)\ny = df_train[\"Survived\"]\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X, y)\n\nimportances = pd.Series(rf.feature_importances_, index=X.columns)\nprint(importances.sort_values(ascending=False).head(15))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:50.768175Z","iopub.execute_input":"2025-09-26T16:13:50.768411Z","iopub.status.idle":"2025-09-26T16:13:50.999183Z","shell.execute_reply.started":"2025-09-26T16:13:50.768393Z","shell.execute_reply":"2025-09-26T16:13:50.998119Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As expected, how much a person paid and as their gender and age plays a big role in whether they survived. I will drop some of the redundant or low importance features.","metadata":{}},{"cell_type":"code","source":"drop_cols = [\n    \"Survived\",\n    \n    # redundant\n    \"IsAlone\",\n    \"AgeBin_Teen\", \"AgeBin_Young Adult\", \"AgeBin_Adult\", \"AgeBin_Senior\",\n    \"FareBin_Medium\", \"FareBin_High\", \"FareBin_Very High\",\n\n    # sparse/low importance\n    \"Cabin_A\", \"Cabin_B\", \"Cabin_C\", \"Cabin_D\", \"Cabin_E\", \n    \"Cabin_F\", \"Cabin_G\", \"Cabin_T\", \"Cabin_U\",\n    \"Embarked_Q\", \"Embarked_S\"\n]\n\nX_train = df_train.drop(columns=[c for c in drop_cols if c in df_train.columns])\ny_train = df_train[\"Survived\"]\n\nX_test  = df_test.drop(columns=[c for c in drop_cols if c in df_test.columns])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:51.001013Z","iopub.execute_input":"2025-09-26T16:13:51.001338Z","iopub.status.idle":"2025-09-26T16:13:51.011711Z","shell.execute_reply.started":"2025-09-26T16:13:51.001309Z","shell.execute_reply":"2025-09-26T16:13:51.010501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X_train.head())\nprint()\nprint(X_test.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:51.012844Z","iopub.execute_input":"2025-09-26T16:13:51.013251Z","iopub.status.idle":"2025-09-26T16:13:51.043255Z","shell.execute_reply.started":"2025-09-26T16:13:51.013228Z","shell.execute_reply":"2025-09-26T16:13:51.042136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"X_train shape:\", X_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"X_test shape:\", X_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:51.044424Z","iopub.execute_input":"2025-09-26T16:13:51.044784Z","iopub.status.idle":"2025-09-26T16:13:51.064061Z","shell.execute_reply.started":"2025-09-26T16:13:51.044753Z","shell.execute_reply":"2025-09-26T16:13:51.063083Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"TARGET_COL            = \"Survived\"\nID_COL                = \"PassengerId\"\nSUBMISSION_TEMPLATE   = \"../input/titanic/gender_submission.csv\"\nSUBMISSION_OUT        = \"submission.csv\"\nSUBMISSION_THRESHOLD  = 0.50\n\nDTYPE_NUMERIC         = \"float32\"\nVAL_SPLIT             = 0.20\nSCALER_CLASS          = \"StandardScaler\"\n\n# MODEL HYPERPARAMS\nH1_UNITS              = 18\nH2_UNITS              = 60\nHIDDEN_ACT            = \"relu\"\nOUTPUT_ACT            = \"sigmoid\"\nKERNEL_INIT           = \"he_uniform\"\nDROPOUT_RATE_1        = 0.50\nDROPOUT_RATE_2        = 0.50\n\n# OPTIMIZER\nSGD_LR                = 0.01\nSGD_MOMENTUM          = 0.90\n\n# TRAINING\nEPOCHS                = 180\nBATCH_SIZE            = 10\nVERBOSE               = 0\nMETRICS               = [\"accuracy\"]\n\n# PLOTTING\nPLOT_TITLE_ACC        = \"Model Accuracy\"\nPLOT_TITLE_LOSS       = \"Model Loss\"\n\nID_COL                = \"PassengerId\"\nDTYPE_NUMERIC         = \"float32\"\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train).astype(DTYPE_NUMERIC)\nX_test_scaled  = scaler.transform(X_test).astype(DTYPE_NUMERIC)\n\nnp.random.seed(42); random.seed(42); tf.random.set_seed(42)\nos.environ[\"PYTHONHASHSEED\"] = \"42\"\n\nN_FEATURES = X_train_scaled.shape[1]\n\nmodel = Sequential([\n    Input(shape=(N_FEATURES,)),\n    Dense(H1_UNITS, activation=HIDDEN_ACT, kernel_initializer=KERNEL_INIT),\n    Dropout(DROPOUT_RATE_1),\n    Dense(H2_UNITS, activation=HIDDEN_ACT, kernel_initializer=KERNEL_INIT),\n    Dropout(DROPOUT_RATE_2),\n    Dense(1, activation=OUTPUT_ACT)\n])\n\noptimizer = SGD(learning_rate=SGD_LR, momentum=SGD_MOMENTUM)\nmodel.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=METRICS)\nmodel.summary()\n\nwith tqdm(total=EPOCHS, desc=\"Training Progress\") as pbar:\n    history = model.fit(\n        X_train_scaled, y_train,\n        validation_split=VAL_SPLIT,\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        verbose=VERBOSE,\n        callbacks=[tf.keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: pbar.update(1))]\n    )\n\nplt.plot(history.history[\"accuracy\"])\nplt.plot(history.history[\"val_accuracy\"])\nplt.title(PLOT_TITLE_ACC)\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"train\", \"val\"], loc=\"upper left\")\nplt.show()\n\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.title(PLOT_TITLE_LOSS)\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"train\", \"val\"], loc=\"upper left\")\nplt.show()\n\ny_pred_proba = model.predict(X_test_scaled, batch_size=BATCH_SIZE).ravel()\ny_pred = (y_pred_proba >= SUBMISSION_THRESHOLD).astype(int)\n\ntrain_scores = model.evaluate(X_train_scaled, y_train, batch_size=BATCH_SIZE, verbose=0)\nprint(f\"{model.metrics_names[1]}: {train_scores[1]*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:13:51.065651Z","iopub.execute_input":"2025-09-26T16:13:51.066030Z","iopub.status.idle":"2025-09-26T16:14:47.203324Z","shell.execute_reply.started":"2025-09-26T16:13:51.065985Z","shell.execute_reply":"2025-09-26T16:14:47.202112Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I experimented with different neural network configurations to improve validation accuracy and reduce overfitting. In my original model, I noticed that the validation accuracy was consistently higher than the training accuracy, which suggested that the heavy dropout layers (set at 0.5 each) were regularizing the model too strongly. To address this, I reduced the dropout rates to 0.3 and also tested variations in network width and depth, such as using wider hidden layers and a shallower architecture with only one dense layer. I also added early stopping with patience of 15 epochs to automatically restore the best weights and prevent unnecessary training once the model stopped improving.\n\nIn addition, I switched from SGD to Adam as the optimizer in some trials. Adam typically converges faster and adapts the learning rate, which is helpful on small datasets like Titanic where quick improvements are visible within fewer epochs. After running these experiments, I compared validation accuracies and selected the configuration that generalized best. These tweaks not only provided me with a more efficient training process but also gave me a better understanding of how dropout, optimizer choice, and model capacity affect performance on structured data tasks.","metadata":{}},{"cell_type":"code","source":"def build_model(n_features, h1=18, h2=60, dr1=0.3, dr2=0.3, opt=\"adam\", lr=1e-3, momentum=0.9):\n    m = Sequential([Input(shape=(n_features,)),\n                    Dense(h1, activation=HIDDEN_ACT, kernel_initializer=KERNEL_INIT),\n                    Dropout(dr1)])\n    if h2 and h2 > 0:\n        m.add(Dense(h2, activation=HIDDEN_ACT, kernel_initializer=KERNEL_INIT))\n        if dr2 and dr2 > 0:\n            m.add(Dropout(dr2))\n    m.add(Dense(1, activation=OUTPUT_ACT))\n    if opt == \"adam\":\n        optimizer = Adam(learning_rate=lr)\n    else:\n        optimizer = SGD(learning_rate=lr, momentum=momentum)\n    m.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=METRICS)\n    return m\n\nexperiments = [\n    {\"name\":\"sgd_dr0.3\",      \"h1\":H1_UNITS, \"h2\":H2_UNITS, \"dr1\":0.30, \"dr2\":0.30, \"opt\":\"sgd\",  \"lr\":SGD_LR, \"momentum\":SGD_MOMENTUM},\n    {\"name\":\"adam_dr0.3\",     \"h1\":H1_UNITS, \"h2\":H2_UNITS, \"dr1\":0.30, \"dr2\":0.30, \"opt\":\"adam\", \"lr\":1e-3},\n    {\"name\":\"adam_wider\",     \"h1\":32,       \"h2\":64,       \"dr1\":0.30, \"dr2\":0.30, \"opt\":\"adam\", \"lr\":1e-3},\n    {\"name\":\"adam_shallow\",   \"h1\":64,       \"h2\":0,        \"dr1\":0.30, \"dr2\":0.00, \"opt\":\"adam\", \"lr\":1e-3},\n]\n\nbest = {\"name\": None, \"val\": -1, \"model\": None, \"history\": None}\nresults = []\n\n\nfor cfg in tqdm(experiments, desc=\"Experiments\"):\n    model_i = build_model(X_train_scaled.shape[1], **{k:v for k,v in cfg.items() if k != \"name\"})\n    es = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=15, restore_best_weights=True)\n    h = model_i.fit(\n        X_train_scaled, y_train,\n        validation_split=VAL_SPLIT,\n        epochs=EPOCHS, batch_size=BATCH_SIZE,\n        verbose=0, callbacks=[es]\n    )\n    best_val = float(np.max(h.history[\"val_accuracy\"]))\n    last_tr  = float(h.history[\"accuracy\"][-1])\n    results.append((cfg[\"name\"], best_val, last_tr, len(h.history[\"loss\"])))\n    if best_val > best[\"val\"]:\n        best.update(name=cfg[\"name\"], val=best_val, model=model_i, history=h)\n\nres_df = pd.DataFrame(results, columns=[\"model\",\"best_val_acc\",\"last_train_acc\",\"epochs_ran\"]).sort_values(\"best_val_acc\", ascending=False)\nprint(res_df)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:14:47.204493Z","iopub.execute_input":"2025-09-26T16:14:47.204775Z","iopub.status.idle":"2025-09-26T16:15:42.971629Z","shell.execute_reply.started":"2025-09-26T16:14:47.204752Z","shell.execute_reply":"2025-09-26T16:15:42.970405Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Results and Analysis","metadata":{}},{"cell_type":"code","source":"print(f\"\\nBest model: {best['name']}  (val_acc={best['val']:.4f})\")\n\nplt.figure()\nplt.plot(best[\"history\"].history[\"accuracy\"], label=\"train\")\nplt.plot(best[\"history\"].history[\"val_accuracy\"], label=\"val\")\nplt.title(PLOT_TITLE_ACC + f\" — {best['name']}\")\nplt.ylabel(\"Accuracy\"); plt.xlabel(\"Epoch\"); plt.legend(); plt.show()\n\nplt.figure()\nplt.plot(best[\"history\"].history[\"loss\"], label=\"train\")\nplt.plot(best[\"history\"].history[\"val_loss\"], label=\"val\")\nplt.title(PLOT_TITLE_LOSS + f\" — {best['name']}\")\nplt.ylabel(\"Loss\"); plt.xlabel(\"Epoch\"); plt.legend(); plt.show()\n\ny_pred_best = (best[\"model\"].predict(X_test_scaled, batch_size=BATCH_SIZE).ravel() >= SUBMISSION_THRESHOLD).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:15:42.976198Z","iopub.execute_input":"2025-09-26T16:15:42.976737Z","iopub.status.idle":"2025-09-26T16:15:43.807124Z","shell.execute_reply.started":"2025-09-26T16:15:42.976701Z","shell.execute_reply":"2025-09-26T16:15:43.805858Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I compared four different neural network setups: reduced dropout with SGD, reduced dropout with Adam, a wider network, and a shallow single-layer model. Both the sgd_dr0.3 and adam_dr0.3 configurations achieved the highest validation accuracy at 0.8883, with Adam requiring more epochs (63 vs. 22) to converge. The adam_shallow model also performed well at 0.8827, showing that even a simpler architecture can capture useful patterns in the Titanic dataset. On the other hand, the adam_wider model underperformed slightly at 0.8659, which suggests that simply adding more capacity did not help and may have led to overfitting or inefficient learning given the small dataset size.\n\nOverall, the results reinforced that dropout tuning and optimizer choice had more impact than network depth or width. Lowering dropout from 0.5 to 0.3 allowed the models to learn more from the data without sacrificing generalization. Adam converged more slowly than SGD here, but both optimizers reached the same peak validation accuracy. Interestingly, the shallow model’s competitive score showed that the Titanic dataset does not require a very deep architecture to achieve strong results. This exercise highlighted the importance of balancing regularization and model complexity, as well as testing optimizer behavior, rather than assuming larger networks will always perform better.","metadata":{}},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"test_ids = pd.read_csv(\"../input/titanic/test.csv\")[ID_COL]\nsubmission_best = pd.DataFrame({ID_COL: test_ids, TARGET_COL: y_pred_best})\nsubmission_path = \"submission.csv\"\nsubmission_best.to_csv(submission_path, index=False)\nprint(\"Saved:\", submission_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:15:43.808372Z","iopub.execute_input":"2025-09-26T16:15:43.808680Z","iopub.status.idle":"2025-09-26T16:15:43.830265Z","shell.execute_reply.started":"2025-09-26T16:15:43.808659Z","shell.execute_reply":"2025-09-26T16:15:43.828935Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Comparison with Supervised/Unsupervised Models","metadata":{}},{"cell_type":"code","source":"clfs = []\nseed = 3\nscaler = (\"Scaler\", StandardScaler())\n\nclfs.append((\"LogReg\",\n             Pipeline([scaler,\n                       (\"LogReg\", LogisticRegression(max_iter=1000, random_state=seed))])))\n\nclfs.append((\"KNN\",\n             Pipeline([scaler,\n                       (\"KNN\", KNeighborsClassifier(n_neighbors=5))])))\n\nclfs.append((\"RidgeClassifier\",\n             Pipeline([scaler,\n                       (\"RidgeClassifier\", RidgeClassifier(random_state=seed))])))\n\n\nclfs.append((\"DecisionTree\",\n             Pipeline([(\"DecisionTree\", DecisionTreeClassifier(random_state=seed))])))\n\nclfs.append((\"RandomForest\",\n             Pipeline([\n                 (\n                     \"RandomForest\",\n                     RandomForestClassifier(\n                         n_estimators=200,\n                         random_state=seed,\n                         n_jobs=1\n                     )\n                 )])))\n\n\nclfs.append((\"GradientBoosting\",\n             Pipeline([(\"GradientBoosting\", GradientBoostingClassifier(\n                 max_features=15, n_estimators=150, random_state=seed))])))\n\nclfs.append((\"ExtraTrees\",\n             Pipeline([(\"ExtraTrees\", ExtraTreesClassifier(\n                 n_estimators=300, random_state=seed, n_jobs=1))])))\n\nclfs.append((\"XGBClassifier\",\n             Pipeline([(\"XGB\", XGBClassifier(\n                 n_estimators=300,\n                 max_depth=4,\n                 subsample=0.9,\n                 colsample_bytree=0.9,\n                 tree_method=\"hist\",\n                 eval_metric=\"logloss\",\n                 random_state=seed,\n                 n_jobs=1\n             ))])))\n\n\nscoring = 'accuracy'\nn_folds = 10\n\nresults, names = [], []\n\nkfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n\nfor name, model in clfs:\n    cv_scores = cross_val_score(\n        model, X_train, y_train,\n        cv=kfold,\n        scoring=scoring,\n        n_jobs=1,\n        error_score=\"raise\"\n    )\n    names.append(name)\n    results.append(cv_scores)\n    print(f\"{name}: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n\ndf_plot = (pd.DataFrame(results, index=names).T\n           .melt(var_name=\"Algorithm\", value_name=\"Accuracy\"))\n\nplt.figure(figsize=(15,6))\nsns.boxplot(data=df_plot, x=\"Algorithm\", y=\"Accuracy\")\nplt.title(\"Classifier Algorithm Comparison\", fontsize=22)\nplt.xlabel(\"Algorithm\", fontsize=20)\nplt.ylabel(\"Accuracy\", fontsize=18)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T16:15:43.831080Z","iopub.execute_input":"2025-09-26T16:15:43.831561Z","iopub.status.idle":"2025-09-26T16:15:57.786257Z","shell.execute_reply.started":"2025-09-26T16:15:43.831531Z","shell.execute_reply":"2025-09-26T16:15:57.785030Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From the cross-validation results, I observed that RidgeClassifier and GradientBoosting were the top-performing models, both with relatively low variance across folds. Logistic Regression and KNN also performed competitively, each scoring above 0.82, showing that even simple linear or distance-based methods can capture meaningful patterns in the Titanic dataset. RandomForest was slightly lower and while still solid, it did not outperform GradientBoosting, which suggests that boosting’s ability to correct mistakes iteratively gave it an edge here.\n\nIn contrast, DecisionTree, ExtraTrees, and XGBoost underperformed relative to the other models. The weaker performance of a single tree is expected given its tendency to overfit, while ExtraTrees and XGBoost may require more hyperparameter tuning to reach their potential. Overall, the results highlight that regularized linear models and boosting methods are particularly effective for this dataset, balancing accuracy with generalization, while simpler ensembles and single-tree models lagged behind.","metadata":{}},{"cell_type":"markdown","source":"# Discussion and Conclusion","metadata":{}},{"cell_type":"markdown","source":"When comparing the supervised algorithms and my neural network runs, I found that the best classical models (RidgeClassifier and GradientBoosting, both around 0.83 CV accuracy) performed nearly on par with my tuned neural networks (which peaked around 0.88 validation accuracy with early stopping). The neural network had a slight edge in terms of validation score, but it required more careful tuning of dropout, optimizer choice, and architecture depth to achieve that result. In contrast, the supervised algorithms delivered strong performance out-of-the-box with much less tuning, showing that for small tabular datasets like Titanic, simpler models can often match or rival neural networks.\n\nCompared to unsupervised approaches I tested earlier (e.g., clustering methods), the supervised models and neural network were clearly superior. The unsupervised models struggled to separate survival outcomes since they lack direct label information, often producing near-random classification metrics. This highlights the importance of using supervised learning when reliable labels are available. Overall, the neural network demonstrated slightly higher potential accuracy, but the classical supervised algorithms were more efficient, stable, and interpretable, while unsupervised methods were unsuitable for this prediction task.","metadata":{}}]}